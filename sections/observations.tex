\subsection{\label{sec:radio}The Radio Lightcurve}
%Image of Raw Data
Radio Observations took place on April 8th, 2024 at the University of Central Arkansas campus in Conway, Arkansas at approximately 35°04'51"N 92°27'36"W with an elevation of 94 m.
We began collecting data at 2460409.2186657293 JD and ended at 2460409.3424835186 JD, or 2024-04-08 17:14:53 UTC and 2024-04-08 20:13:11 UTC.
The telescope used was a Spider 230C radio telescope, a 2.3 m parabolic dish tuned to the electron spin-flip transition of Hydrogen at 1420 MHz.
During observation, we encountered minimal cloud cover and no sustained winds.
Despite this, we did have 3 sources of error: A strong gust of wind pushed the telescope off target for a few seconds, an issue with the accuracy of our polar alignment caused the telescope to be slightly off target by the end of observation, and local sources of interference created noise in the data.
In order for this raw data to be compared to the optical lightcurve, these errors must be addressed.
A number of data analysis methods were used to do this:
%%Image of linear adjustment functino being added to raw data?
\paragraph{Linear Adjustment function}
The first step in our data analysis is to correct for the tracking error.
We found average Analog to Digital Unit (ADU) counts for the duration of time after 4th contact and average ADU counts for the same duration of time before 1st contact.
We then created a linear function that shifts the data at the end of the lightcurve upward and decreases in effect to 0 at 1st contact.
%Image of smoothing being done to above image?
\paragraph{Smoothing}
The next step is to smooth the data.
Our choice to smooth the data came from the need to reduce noise/random spikes in the data to get a better idea of the shape of the curve.
We chose to apply a Savitzky-Golay filter to do this.
%Image of normalization being done to above image?
\paragraph{Normalization}
The final step is to normalize the data.
Normalization is the process by which the data of a graph is scaled by a constant such that the highest points are equal to 100, essentially making the data a percentage of its highest value.
This is done after smoothing to avoid issues with noise spikes.
Converting the ADU counts to a relative brightness makes it possible to compare with the optical lightcurves.
\\
Now that the raw data has been refined, a neat radio lightcurve of the eclipse remains.
\subsection{\label{sec:optical}The Optical Lightcurve}
We collected optical lightcurve data to compare contact times, the depth of the eclipse, and the shape of the lightcurve to our radio observations. Our two sources include a livestream lightcurve using a solar filter-equipped optical telescope, and a Vernier Light Sensor (LS-BTA) pointed straight up at the sky.
\paragraph{The Livestream Lightcurve}
The livestream of the solar eclipse was created using a Sony Alpha 6400 camera with a Sony FE 24mm-240mm f/3.5-6.3 lens and NiSi Solar Filter Pro Nan UV/IR Cut ND 100000 72mm filter.
This solar filter was removed during totality, marked by 2460409.2849 JD and 2460409.2884 JD.
This setup was attached to a Meade-14 LX200R on a Paramount MX+ equatorial mount for target tracking.
The lightcurve for this data was created by grayscaling the image of the eclipse and using Otsu thresholding to only count pixels above a certain value.
To get the time for each data point, we took an image of the in-video clock at the same time we counted the pixels of the eclipse and used computer vision to recognise the characters in the clock, giving us the time at which the frame was pixel-counted.
The number of pixels of the solar disk over time can then be normalized to a relative brightness.
We compared this livestream lightcurve to a theoretical lightcurve generated by the software \texttt{Stellarium}\cite{zotti_simulated_2020}, and found that the livestream and theoretical lightcurves overlap well.
\paragraph{The Light Sensor Lightcurve}
The light sensor used to collect data during the solar eclipse was the Vernier Light Sensor (LS-BTA) pointed at the sky.
Dr. Todd Abel and Shane Ayotte provided the light sensor data, which we normalized to relative brightness.
Because the light sensor data had timestamps included, we used it to confirm the timing accuracy of our other lightcurves.
\\
By stacking the lightcurves, we observed that all three had a very similar shape, with heavy overlap between the theoretical and livestream lightcurves.
This implies that our lightcurves are accurate and can be used to compare with our radio lightcurve to determine the size of the sun’s radio emission.
\\
\begin{figure}
    \includegraphics[width=0.5\textwidth]{figures/radioOpticalComparison}
    \caption{\label{fig:radioOpticalComparison} A comparison of the smoothed/normalized radio lightcurve and the normalized optical lightcurve generated from the livestream.}
\end{figure}